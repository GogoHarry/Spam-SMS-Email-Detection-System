This is the code that was used to trained the model
"# Import models from scikit learn module
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import KFold  # Updated import for KFold
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier, export_graphviz
from sklearn import metrics

# Generic function for making a classification model and assessing performance
def classification_model(model, data, predictors, outcome):
    
    # Fit the model
    model.fit(data[predictors], data[outcome])
  
    # Make predictions on the training set
    predictions = model.predict(data[predictors])
  
    # Print accuracy
    accuracy = metrics.accuracy_score(predictions, data[outcome])
    print("Accuracy : %s" % "{0:.3%}".format(accuracy))

    # Perform k-fold cross-validation with 5 folds
    kf = KFold(n_splits=5, shuffle=True, random_state=1)
    error = []
    for train_index, test_index in kf.split(data):
        # Filter training data
        train_predictors = data[predictors].iloc[train_index, :]
    
        # The target we're using to train the algorithm
        train_target = data[outcome].iloc[train_index]
  
        # Training the algorithm using the predictors and target
        model.fit(train_predictors, train_target)
  
        # Record error from each cross-validation run
        error.append(model.score(data[predictors].iloc[test_index, :], data[outcome].iloc[test_index]))
 
    print("Cross-Validation Score : %s" % "{0:.3%}".format(np.mean(error)))

    # Fit the model again so that it can be referred outside the function
    model.fit(data[predictors], data[outcome])
"

from xgboost import XGBClassifier

# Initialize the XGBClassifier with specific parameters
model = XGBClassifier(
    objective='binary:logistic',  # for binary classification
    max_depth=3,  # maximum depth of the tree
    learning_rate=0.1,  # learning rate
    n_estimators=100,  # number of trees
    min_child_weight=1,  # minimum sum of instance weight (hessian) needed in a child
    gamma=0,  # minimum loss reduction required to make a further partition
    subsample=0.8,  # subsample ratio of the training instances
    colsample_bytree=0.8,  # subsample ratio of columns when constructing each tree
    reg_alpha=0,  # L1 regularization term on weights
    reg_lambda=1  # L2 regularization term on weights
)

# List of predictor variables
predictor_var = ['Credit_History', 'TotalIncome_log', 'LoanAmount_log', 'Dependents', 'Property_Area']
# Assuming classification_model is a custom function defined elsewhere
classification_model(model, df, predictor_var, outcome_var)
